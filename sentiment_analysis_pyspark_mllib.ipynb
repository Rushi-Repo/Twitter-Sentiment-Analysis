{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4e91d34768ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0mspark_home\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[1;34m\"Couldn't find Spark, make sure SPARK_HOME env is set\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;34m\" or Spark is in an expected location (e.g. from homebrew installation).\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf=SparkConf().setAppName(\"BigData_final\").setMaster(\"local\")\n",
    "sc=SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.feature import IDF\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, LogisticRegressionModel\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.mllib.feature import ChiSqSelector\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import os, tempfile\n",
    "import csv\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_values_from_list(the_list, val):\n",
    "    return [value for value in the_list if value != val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "\n",
    "def clean(filename):\n",
    "    text=open(filename, encoding='ISO-8859-1')\n",
    "    file_reader = csv.reader(text)\n",
    "    clean_final=[]\n",
    "    c=0\n",
    "    stop=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "    shuffle=[]\n",
    "    tweet_list=[]\n",
    "    for row in file_reader:\n",
    "        temp_t=row[5]\n",
    "        ts=temp_t.split(\" \")\n",
    "        for w in range(0,len(ts)):\n",
    "            ts[w]=ts[w].lower()\n",
    "            if (ts[w]!='' and ts[w][0]=='@'):\n",
    "                ts[w]='AT_USER'\n",
    "            if (ts[w]!='' and len(ts[w])>4  and ts[w][0]=='w' and ts[w][1]=='w' and ts[w][2]=='w' and ts[w][3]=='.'):\n",
    "                ts[w]='URL'\n",
    "            if (ts[w]!='' and len(ts[w])>7  and ts[w][0]=='h' and ts[w][1]=='t' and ts[w][2]=='t' and ts[w][3]=='p' and\n",
    "                ts[w][4]==':' and ts[w][5]=='/' and ts[w][6]=='/'):\n",
    "                ts[w]='URL'\n",
    "            if (ts[w]!='' and len(ts[w])>8  and ts[w][0]=='h' and ts[w][1]=='t' and ts[w][2]=='t' and ts[w][3]=='p' and\n",
    "                ts[w][4]=='s' and ts[w][5]==':' and ts[w][6]=='/' and ts[w][7]=='/'):\n",
    "                ts[w]='URL'\n",
    "            for i in range(0,len(ts[w])):\n",
    "                flag=0\n",
    "                if len(ts[w])>i+2 and ts[w][i]==ts[w][i+1] and ts[w][i]==ts[w][i+2]:\n",
    "                    for j in range(i+2,len(ts[w])):\n",
    "                        if ts[w][i]==ts[w][j]:\n",
    "                            flag=1\n",
    "                            if len(ts[w])>j+1 and ts[w][i]!=ts[w][j+1]:\n",
    "                                break\n",
    "                        if j==(len(ts[w]))-1 and ts[w][i]==ts[w][j]:\n",
    "                            flag=1\n",
    "                            break\n",
    "                    if flag==1:\n",
    "                        ts[w]=ts[w].replace(ts[w][i:j+1],ts[w][i])\n",
    "            if (ts[w]!='' and ts[w][0].isdigit()):\n",
    "                ts[w]=''\n",
    "        for i in range(0,len(stop)):\n",
    "            if stop[i] in ts:\n",
    "                ts = remove_values_from_list(ts, stop[i])\n",
    "        cleaned_list=[word.strip(string.punctuation) for word in ts]\n",
    "        while '' in cleaned_list:\n",
    "            cleaned_list.remove('')\n",
    "        clean_final.append(cleaned_list)\n",
    "        shuffle.append([row[0],cleaned_list])\n",
    "        tweet_list.append(row[5])\n",
    "    return (shuffle,tweet_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompTF(tshuffle_rdd):\n",
    "    t_rdd=sc.parallelize([row[1] for row in tshuffle_rdd.collect()])\n",
    "    hashingTF = HashingTF(25000)\n",
    "    tf = hashingTF.transform(t_rdd)\n",
    "    return tf\n",
    "\n",
    "def CompTF_withNumFeatures(tshuffle_rdd):\n",
    "    t_rdd=sc.parallelize([row[1] for row in tshuffle_rdd.collect()])\n",
    "    hashingTF = HashingTF(1500)\n",
    "    tf = hashingTF.transform(t_rdd)\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompIDF(tf):\n",
    "    tf.cache()\n",
    "    idf = IDF().fit(tf)\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompTFIDF(tf,idf):\n",
    "    tfidf = idf.transform(tf)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting meaningful content\n",
    "\n",
    "def Convert_to_LabeledPoint(labels,features):\n",
    "    training = labels.zip(features).map(lambda x: LabeledPoint(x[0], x[1]))\n",
    "    return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing NB\n",
    "\n",
    "\n",
    "def NB_train(training):\n",
    "    model = NaiveBayes.train(training)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing logistic regression\n",
    "\n",
    "def LG_train(training):\n",
    "    model = LogisticRegressionWithSGD.train(training,regType='l2')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "\n",
    "def test(model,labels,features):\n",
    "    labels_and_preds = labels.zip(model.predict(features)).map(\n",
    "                                lambda x: {\"actual\": float(x[0]), \"predicted\": float(x[1])})\n",
    "    acc=100.0*((labels_and_preds.filter(lambda x:x[\"actual\"]==x[\"predicted\"]).count()) / (labels.count()))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def test_wpp(model,labels,features):\n",
    "    labels_and_probs = labels.zip(model.predict(features)).map(\n",
    "                                lambda x: {\"actual\": float(x[0]), \"probs\": float(x[1])})\n",
    "    return labels_and_probs\n",
    "\n",
    "#Final Test\n",
    "\n",
    "def test_final(model,labels,features):\n",
    "    labels_and_preds = labels.zip(model.predict(features)).map(\n",
    "                                lambda x: {\"actual\": float(x[0]), \"predicted\": float(x[1])})\n",
    "    acc=100.0*((labels_and_preds.filter(lambda x:x[\"actual\"]==x[\"predicted\"]).count()) / (labels.count()))\n",
    "    return (labels_and_preds,acc)\n",
    "\n",
    "def tf(labels_and_preds):\n",
    "    true_pos = (labels_and_preds.filter(lambda x:x[\"predicted\"]==1 and x[\"actual\"]==1).count())\n",
    "    true_neg = (labels_and_preds.filter(lambda x:x[\"predicted\"]==0 and x[\"actual\"]==0).count())\n",
    "    false_pos = (labels_and_preds.filter(lambda x:x[\"predicted\"]==1 and x[\"actual\"]==0).count())\n",
    "    false_neg = (labels_and_preds.filter(lambda x:x[\"predicted\"]==0 and x[\"actual\"]==1).count())\n",
    "\n",
    "    tpr=(true_pos)/(true_pos+false_neg)\n",
    "    fpr=(false_pos)/(false_pos+true_neg)\n",
    "    return (tpr,fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'clean' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ea3811020e8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_shuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_tweet_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training_dataset.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_shuffle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtshuff_rdd_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_shuffle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean' is not defined"
     ]
    }
   ],
   "source": [
    "   \n",
    "train_shuffle,train_tweet_list=clean(\"training_dataset.csv\")\n",
    "\n",
    "random.shuffle(train_shuffle)\n",
    "\n",
    "tshuff_rdd_train=sc.parallelize(train_shuffle)\n",
    "\n",
    "p=tshuff_rdd_train.randomSplit(weights=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], seed=1)\n",
    "\n",
    "\n",
    "test_shuffle,test_tweet_list=clean(\"testing_dataset.csv\")\n",
    "print(\"training and testing done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy of Training data\n",
    "\n",
    "tf_train=CompTF(tshuff_rdd_train)\n",
    "idf_train=CompIDF(tf_train)\n",
    "tfidf_train=CompTFIDF(tf_train,idf_train)\n",
    "\n",
    "\n",
    "#Feature Extraction for training (train.csv) dataset\n",
    "\n",
    "training_NB = Convert_to_LabeledPoint(sc.parallelize([row[0] for row in tshuff_rdd_train.collect()]),tfidf_train)\n",
    "\n",
    "\n",
    "#training accuracy for NB ML technique - training with training data\n",
    "\n",
    "model_train_NB=NB_train(training_NB)\n",
    "\n",
    "\n",
    "#testing\n",
    "\n",
    "accuracy_NB=test(model_train_NB,sc.parallelize([row[0] for row in tshuff_rdd_train.collect()]),tfidf_train)\n",
    "\n",
    "print (\"TRAINING ACCURACY:-\\n\")\n",
    "\n",
    "print(\"The accuracy for the training dataset tested on the training data itself using NB is\",accuracy_NB,\"%\")\n",
    "\n",
    "print (\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"10-FOLD CV ACCURACIES FOR ALL ITERATIONS\\n\")\n",
    "\n",
    "\n",
    "tot_NB_kfold=0\n",
    "NB_kfold_set=[]\n",
    "for i in range(0,len(p)):\n",
    "    test_RDD=p[i]\n",
    "    train_tempRDD=sc.emptyRDD()\n",
    "    for j in range(0,len(p)):\n",
    "        if i!=j:\n",
    "            train_tempRDD=train_tempRDD.union(p[j])\n",
    "    tf_train=CompTF(train_tempRDD)\n",
    "    idf_train=CompIDF(tf_train)\n",
    "    tfidf_train=CompTFIDF(tf_train,idf_train)\n",
    "    training = Convert_to_LabeledPoint(sc.parallelize([row[0] for row in train_tempRDD.collect()]),tfidf_train)\n",
    "    model_train=NB_train(training)\n",
    "    tf_test=CompTF(test_RDD)\n",
    "    tfidf_test=CompTFIDF(tf_test,idf_train)\n",
    "    accuracy=test(model_train,sc.parallelize([row[0] for row in test_RDD.collect()]),tfidf_test)\n",
    "    print (\"The accuracy for number\",i+1,\"kth partition test for 10-fold cross validation for NB is\",accuracy,\"%\")\n",
    "    NB_kfold_set.append(accuracy)\n",
    "    tot_NB_kfold=tot_NB_kfold+accuracy\n",
    "avg_NB_kfold=tot_NB_kfold/len(p)\n",
    "NB_kfold_best=max(NB_kfold_set)\n",
    "print (\"\\n\")\n",
    "print (\"The average accuracy for NB after 10-fold cross validation is\",avg_NB_kfold,\"%\")\n",
    "print (\"\\n\")\n",
    "print (\"The highest accuracy for NB after 10-fold cross validation is\",NB_kfold_best,\"%\")\n",
    "print (\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tshuff_rdd_test=sc.parallelize(test_shuffle)\n",
    "tf_test=CompTF(tshuff_rdd_test)\n",
    "tf_train=CompTF(tshuff_rdd_train)\n",
    "idf_train=CompIDF(tf_train)\n",
    "tfidf_test=CompTFIDF(tf_test,idf_train)\n",
    "\n",
    "\n",
    "\n",
    "labels_and_preds_NB,accu_NB = test_final(model_train_NB,sc.parallelize([row[0] for row in tshuff_rdd_test.collect()]),tfidf_test)\n",
    "metrics2 = BinaryClassificationMetrics(labels_and_preds_NB.map(lambda x: (x[\"predicted\"], x[\"actual\"])))\n",
    "print (\"\\nTEST ACCURACY:-\\n\")\n",
    "print(\"The accuracy of prediction for NB on testing data is\",accu_NB,\"%\")\n",
    "objects = ('Training Accuracy', '10-Fold CV', 'Testing Accuracy')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [accuracy_NB,avg_NB_kfold,accu_NB]\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.xlabel('Classifications')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Naive Bayes Classifier - Accuracies')\n",
    " \n",
    "plt.show()\n",
    "\n",
    "metrics = MulticlassMetrics(labels_and_preds_NB.map(lambda x: (x[\"predicted\"], x[\"actual\"])))\n",
    "\n",
    "print(\"\\nSummary Stats_NB\\n\")\n",
    "\n",
    "labels = (sc.parallelize([row[0] for row in tshuff_rdd_test.collect()])).distinct().collect()\n",
    "for label in sorted(labels):\n",
    "    print(\"Class %s precision_NB = %s\" % (label, metrics.precision(label)))\n",
    "    print(\"Class %s recall_NB = %s\" % (label, metrics.recall(label)))\n",
    "    print(\"Class %s F1 Measure_NB = %s\" % (label, metrics.fMeasure(float(label), beta=1.0)))\n",
    "\n",
    "print(\"\\nAvg/Weighted recall_NB = %s\" % metrics.weightedRecall)\n",
    "print(\"Avg/Weighted precision_NB = %s\" % metrics.weightedPrecision)\n",
    "print(\"Avg/Weighted F(1) Score_NB = %s\" % metrics.weightedFMeasure())\n",
    "cm=metrics.confusionMatrix().toArray()\n",
    "print(\"\\nConfusion matrix_NB=\")\n",
    "print(cm)\n",
    "print(\"\\n\")\n",
    "metrics2 = BinaryClassificationMetrics(labels_and_preds_NB.map(lambda x: (x[\"predicted\"], x[\"actual\"])))\n",
    "print(\"Area under ROC_NB = %s\" % metrics2.areaUnderROC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tshuff_rdd_test=sc.parallelize(test_shuffle)\n",
    "tf_test=CompTF(tshuff_rdd_test)\n",
    "tf_train=CompTF(tshuff_rdd_train)\n",
    "idf_train=CompIDF(tf_train)\n",
    "tfidf_test=CompTFIDF(tf_test,idf_train)\n",
    "\n",
    "\n",
    "\n",
    "labels_and_preds_NB,accu_NB = test_final(model_train_NB,sc.parallelize([row[0] for row in tshuff_rdd_test.collect()]),tfidf_test)\n",
    "metrics2 = BinaryClassificationMetrics(labels_and_preds_NB.map(lambda x: (x[\"predicted\"], x[\"actual\"])))\n",
    "print (\"\\nTEST ACCURACY:-\\n\")\n",
    "print(\"The accuracy of prediction for NB on testing data is\",accu_NB,\"%\")\n",
    "objects = ('Training Accuracy', '10-Fold CV', 'Testing Accuracy')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [accuracy_NB,avg_NB_kfold,accu_NB]\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.xlabel('Classifications')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Naive Bayes Classifier - Accuracies')\n",
    " \n",
    "plt.show()\n",
    "\n",
    "metrics = MulticlassMetrics(labels_and_preds_NB.map(lambda x: (x[\"predicted\"], x[\"actual\"])))\n",
    "\n",
    "print(\"\\nSummary Stats_NB\\n\")\n",
    "\n",
    "labels = (sc.parallelize([row[0] for row in tshuff_rdd_test.collect()])).distinct().collect()\n",
    "for label in sorted(labels):\n",
    "    print(\"Class %s precision_NB = %s\" % (label, metrics.precision(label)))\n",
    "    print(\"Class %s recall_NB = %s\" % (label, metrics.recall(label)))\n",
    "    print(\"Class %s F1 Measure_NB = %s\" % (label, metrics.fMeasure(float(label), beta=1.0)))\n",
    "\n",
    "\n",
    "print(\"\\nAvg/Weighted recall_NB = %s\" % metrics.weightedRecall)\n",
    "print(\"Avg/Weighted precision_NB = %s\" % metrics.weightedPrecision)\n",
    "print(\"Avg/Weighted F(1) Score_NB = %s\" % metrics.weightedFMeasure())\n",
    "\n",
    "cm=metrics.confusionMatrix().toArray()\n",
    "print(\"\\nConfusion matrix_NB=\")\n",
    "print(cm)\n",
    "print(\"\\n\")\n",
    "metrics2 = BinaryClassificationMetrics(labels_and_preds_NB.map(lambda x: (x[\"predicted\"], x[\"actual\"])))\n",
    "\n",
    "\n",
    "print(\"Area under ROC_NB = %s\" % metrics2.areaUnderROC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats\n",
    "\n",
    "tf_train=CompTF(tshuff_rdd_train)\n",
    "idf_train=CompIDF(tf_train)\n",
    "tfidf_train=CompTFIDF(tf_train,idf_train)\n",
    "training = Convert_to_LabeledPoint(sc.parallelize([row[0] for row in tshuff_rdd_train.collect()]),tfidf_train)\n",
    "model_train_LG=LG_train(training)\n",
    "accuracy=test(model_train_LG,sc.parallelize([row[0] for row in tshuff_rdd_train.collect()]),tfidf_train)\n",
    "print (\"TRAINING ACCURACY:-\\n\")\n",
    "print(\"The accuracy for the training dataset\",accuracy,\"%\")\n",
    "print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rdd.take(10):   print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8d3513b7698b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python395jvsc74a57bd0dd0f17d9a7d0d927da5dbf22a1d3030668f4fb2eba245ea5be983aa804c7aca2",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "dd0f17d9a7d0d927da5dbf22a1d3030668f4fb2eba245ea5be983aa804c7aca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}